{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d08766",
   "metadata": {},
   "source": [
    "### Implementing Linear regression using Batch Gradient descent on 2x2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc1bf361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a13e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our data set\n",
    "x_train = np.array([1.0, 2.0])   #features\n",
    "y_train = np.array([300.0, 500.0])   #target value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea74b743",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" computing the cost\n",
    "now to calculate cost, we need: \n",
    "1. m - the number of data points\n",
    "2. y^ - predicted value (formula: w.xi + b)\n",
    "3. y - target value\n",
    "4. x - input value\n",
    "4. w - weight\n",
    "5. b - bias\n",
    "\"\"\"\n",
    "def compute_cost(w,b,y,x):\n",
    "    \n",
    "    # number of examples we will loop (since its summation)\n",
    "    m = x.shape[0]\n",
    "    cost = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i]) ** 2\n",
    "    total_cost = 1 / (2*m) * cost\n",
    "\n",
    "    return total_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8970083b",
   "metadata": {},
   "source": [
    "Now we have the compute_cost function, which will help us compute the cost of any w or b.\n",
    "\n",
    "Next, we need to calculate gradient, so that we can update w and b to minimize the error and get the best possible solution.\n",
    "\n",
    "To calculate the gradient and update w and b, we need:\n",
    "1. learning rate\n",
    "2. w_old\n",
    "3. pde of cost wrt w - which is the gradient\n",
    "\n",
    "so first we will create a function: compute_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdcd683",
   "metadata": {},
   "source": [
    "\n",
    "*gradient descent* was described as:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "where, parameters $w$, $b$ are updated simultaneously.  \n",
    "The gradient is defined as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\n",
    "\\frac{\\partial J}{\\partial w}\n",
    "= \\frac{1}{m} \\sum_{i=1}^{m} ( \\hat{y}_i - y_i ) x_i\n",
    "$\n",
    "\n",
    "\n",
    "Here *simultaniously* means that you calculate the partial derivatives for all the parameters before updating any of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "697fd1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "calculate the gradient\n",
    "\"\"\"\n",
    "def compute_gradient(w,b,x,y):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        dj_dw_i = (f_wb - y[i]) * x[i]\n",
    "        dj_db_i = (f_wb - y[i])\n",
    "        dj_dw += dj_dw_i\n",
    "        dj_db += dj_db_i\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350107c4",
   "metadata": {},
   "source": [
    "Now we have the function to calculate the gradient, and now we will move forward to calculate the descent and update w and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ad9c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(compute_cost, compute_gradient, w_in, b_in, x, y, alpha, num_iters):\n",
    "    \"\"\"\n",
    "        w_in = initial weight \n",
    "        b_in = initial bias\n",
    "        alpha = learning rate (hyperparam)\n",
    "        compute_cost = function to compute cost using MSE\n",
    "        compute_gradient = function to compute gradient and give the value of dj_dw and dj_db\n",
    "        x = training input values\n",
    "        y = training target values\n",
    "        num_iters = number of iterations the gradient descent will run, since this is batch gd, we can also call it epochs (as it sees the entire data n number of times)\n",
    "\n",
    "    Returns:\n",
    "        w_new = updated w\n",
    "        b_new = updated b\n",
    "        J_history = history of costs\n",
    "        p_history = history of params (w and b)\n",
    "    \"\"\"\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    w = w_in\n",
    "    b = b_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # w_new = w_old - alpha * gradient\n",
    "        dj_dw, dj_db = compute_gradient(w,b,x,y)\n",
    "\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        J_history.append(compute_cost(w,b,x,y))\n",
    "        p_history.append([w,b])\n",
    "\n",
    "    return w, b, J_history, p_history\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5daacfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1000\n",
    "w_final,b_final,J_his, p_his = gradient_descent(compute_cost, compute_gradient, 0, 0, x_train, y_train, 0.1, iterations)\n",
    "print(\"Final weight:\", w_final)\n",
    "print(\"Final bias:\", b_final)\n",
    "\n",
    "for i in range(iterations):\n",
    "    print(J_his[i], p_his[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26af5868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 sqft house prediction 300.0 Thousand dollars\n",
      "1200 sqft house prediction 340.0 Thousand dollars\n",
      "2000 sqft house prediction 500.0 Thousand dollars\n"
     ]
    }
   ],
   "source": [
    "print(f\"1000 sqft house prediction {w_final*1.0 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"1200 sqft house prediction {w_final*1.2 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"2000 sqft house prediction {w_final*2.0 + b_final:0.1f} Thousand dollars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd386f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
